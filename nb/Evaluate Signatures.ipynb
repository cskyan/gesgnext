{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, psutil, difflib\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import gseapy as gp\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from bionlp.util import fs, io, func, njobs\n",
    "from bionlp import txtclf\n",
    "\n",
    "LABEL2IDX = {'gene perturbation':2, 'drug perturbation':1, 'disease signature':0}\n",
    "LABEL2OBJ = {'gene perturbation':'hs_gene_symbol', 'drug perturbation':'drug_name', 'disease signature':'disease_name'}\n",
    "RUN_LABEL = 'drug perturbation'\n",
    "_RUN_LABEL = RUN_LABEL.replace(' ', '_')\n",
    "DGE_METHOD = 'limma-fdr'\n",
    "DATA_PATH = '../../data/gesgnext'\n",
    "GE_PATH = '../../data/gesgnext/gedata/%s' % _RUN_LABEL\n",
    "DGE_PATH = '../../data/gesgnext/dge/%s/%s' % (DGE_METHOD, _RUN_LABEL)\n",
    "DGE_DATA_PATH = '../../data/gesgnext/dge/%s' % _RUN_LABEL\n",
    "# DGE_CACHE_PATH = '../../data/gesgnext/dge/cache/%s/%s' % (_RUN_LABEL, DGE_METHOD)\n",
    "GEO_PATH = '../../data/gesgnext/geo'\n",
    "GSE_DIR = '../../data/gesgnext/geo/xml/%s' % _RUN_LABEL\n",
    "SAMP_DIR = '../../data/gesgnext/geo/xml/%s/samples' % _RUN_LABEL\n",
    "PLATFORM_PATH = '../../data/gesgnext/geo/xml/%s/platforms' % _RUN_LABEL\n",
    "SGNDB_PATH = '../../data/gesgnext/sgndb/%s' % _RUN_LABEL\n",
    "WIKIPATHWAYS_PATH = '../../data/gesgnext/wikipathways'\n",
    "\n",
    "# probe_gene_map = io.read_obj(os.path.join(PLATFORM_PATH, 'probe_gene_map.pkl'))\n",
    "probe_gene_map = None\n",
    "SGN_MIN_SIZE, SGN_MAX_SIZE = 5, 100\n",
    "SC=' /// '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and Construct Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signatures\n",
    "# sgn_df = pd.read_csv(os.path.join(DATA_PATH, '%s.csv'%RUN_LABEL.replace(' ', '_')))\n",
    "sgn_df = pd.read_excel(os.path.join(DATA_PATH, '%s.xlsx'%RUN_LABEL.replace(' ', '_')))\n",
    "# Differential gene expression\n",
    "# dge_dfs = [io.read_df(os.path.join(DGE_PATH, fname), with_idx=True) for fname in ['dge_%s.npz'%x for x in range(sgn_df.shape[0])]]\n",
    "dge_dfs = [io.read_df(os.path.join(DGE_PATH, fname), with_idx=True) for fname in ['dge_%s.npz'%sgn_id for sgn_id in sgn_df['id']]]\n",
    "# dge_dfs = [io.read_df(os.path.join(DGE_PATH, 'dge_%s.npz'%sgn_id.split(':')[-1]), with_idx=True) for sgn_id in sgn_df['id']]\n",
    "# dge_dfs = [io.read_df(os.path.join(DGE_CACHE_PATH, '%s.npz'%sgn_id)) for sgn_id in sgn_df['id']]\n",
    "\n",
    "for geo_id, sgn_ids in sgn_df.groupby('geo_id').groups.iteritems():\n",
    "    # Training data for classifier\n",
    "    sub_sgn_df = sgn_df.loc[sgn_ids]\n",
    "    sub_dge_dfs = [dge_dfs[i] for i in sgn_ids]\n",
    "    dge_X = pd.concat([dge_df['statistic'].to_frame() for dge_df in sub_dge_dfs], axis=1, join='inner')\n",
    "#     dge_X = pd.concat([dge_df['t'].to_frame() for dge_df in sub_dge_dfs], axis=1, join='inner')\n",
    "    dge_X.columns = sub_sgn_df['id']\n",
    "    dge_X = dge_X.transpose()\n",
    "    io.write_df(dge_X, os.path.join(DGE_DATA_PATH, 'dge_X_%s.npz'%geo_id), with_idx=True, compress=True)\n",
    "    # Label Construction\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    bin_label = (mlb.fit_transform(sub_sgn_df[LABEL2OBJ[RUN_LABEL]].apply(str).as_matrix().reshape(-1,1)), mlb.classes_)\n",
    "    io.write_df(pd.DataFrame(bin_label[0], index=dge_X.index, columns=bin_label[1]), os.path.join(DGE_DATA_PATH, 'dge_Y_%s.npz'%geo_id), with_idx=True, sparse_fmt='csr', compress=True)\n",
    "    le = LabelEncoder()\n",
    "    encoded_lb = (le.fit_transform(sub_sgn_df[LABEL2OBJ[RUN_LABEL]].apply(str).as_matrix()), le.classes_)\n",
    "    io.write_df(pd.DataFrame(encoded_lb[0], index=dge_X.index, columns=[';'.join(['%i:%s'%(i,x) for i, x in enumerate(encoded_lb[1])])]), os.path.join(DGE_DATA_PATH, 'dge_ecY_%s.npz'%geo_id), with_idx=True, compress=True)\n",
    "    del dge_X, bin_label, encoded_lb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and Construct Data Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgn2dgeg(groups, sgn_df, dge_dir, dgeg_dir):\n",
    "    for geo_id, sgn_ids in groups:\n",
    "        # Training data for classifier\n",
    "        sub_sgn_df = sgn_df.loc[sgn_ids]\n",
    "#         sub_dge_dfs = [dge_dfs[i] for i in sgn_ids]\n",
    "        sub_dge_dfs = [io.read_df(os.path.join(dge_dir, fname), with_idx=True) for fname in ['dge_%s.npz'%sgn_id for sgn_id in sub_sgn_df['id']]]\n",
    "        dge_X = pd.concat([dge_df['statistic'].to_frame() for dge_df in sub_dge_dfs], axis=1, join='inner')\n",
    "        dge_X.columns = sub_sgn_df['id']\n",
    "        dge_X = dge_X.transpose()\n",
    "        io.write_df(dge_X, os.path.join(dgeg_dir, 'dge_X_%s.npz'%geo_id), with_idx=True, compress=True)\n",
    "        # Label Construction\n",
    "        mlb = MultiLabelBinarizer()\n",
    "        bin_label = (mlb.fit_transform(sub_sgn_df[LABEL2OBJ[RUN_LABEL]].apply(str).as_matrix().reshape(-1,1)), mlb.classes_)\n",
    "        io.write_df(pd.DataFrame(bin_label[0], index=dge_X.index, columns=bin_label[1]), os.path.join(dgeg_dir, 'dge_Y_%s.npz'%geo_id), with_idx=True, sparse_fmt='csr', compress=True)\n",
    "        le = LabelEncoder()\n",
    "        encoded_lb = (le.fit_transform(sub_sgn_df[LABEL2OBJ[RUN_LABEL]].apply(str).as_matrix()), le.classes_)\n",
    "        io.write_df(pd.DataFrame(encoded_lb[0], index=dge_X.index, columns=[';'.join(['%i:%s'%(i,x) for i, x in enumerate(encoded_lb[1])])]), os.path.join(DGE_DATA_PATH, 'dge_ecY_%s.npz'%geo_id), with_idx=True, compress=True)\n",
    "        del dge_X, bin_label, encoded_lb\n",
    "\n",
    "\n",
    "sgn_df = pd.read_excel(os.path.join(DATA_PATH, '%s.xlsx'%RUN_LABEL.replace(' ', '_')))\n",
    "\n",
    "groups = sgn_df.groupby('geo_id').groups.items()\n",
    "numprocs = psutil.cpu_count()\n",
    "task_bnd = njobs.split_1d(len(groups), split_num=numprocs, ret_idx=True)\n",
    "_ = njobs.run_pool(sgn2dgeg, n_jobs=numprocs, dist_param=['groups'], groups=[groups[task_bnd[i]:task_bnd[i+1]] for i in range(numprocs)], sgn_df=sgn_df, dge_dir=DGE_PATH, dgeg_dir=DGE_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Gene Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sgn_df = pd.read_excel(os.path.join(DATA_PATH, '%s.xlsx' % _RUN_LABEL))\n",
    "idx_sgn_df = sgn_df.set_index('id')\n",
    "probe_gene_map = io.read_obj(os.path.join(PLATFORM_PATH, 'probe_gene_map.pkl'))\n",
    "keep_unkown_probe, hist_bnd = False, (-2, 1)\n",
    "udr_genes = []\n",
    "for dge_X_fpath in fs.listf(DGE_DATA_PATH, pattern='dge_X_.*\\.npz', full_path=True):\n",
    "    dge_X = io.read_df(dge_X_fpath, with_idx=True).replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    if (np.all(pd.isnull(dge_X.as_matrix()))): continue\n",
    "    # Filter out the probes that cannot be converted to gene symbols\n",
    "    plfm = idx_sgn_df['platform'].loc[dge_X.index[0]]\n",
    "    has_plfm = probe_gene_map and probe_gene_map.has_key(plfm) and not probe_gene_map[plfm].empty\n",
    "    if (has_plfm and not keep_unkown_probe):\n",
    "        pgmap = probe_gene_map[plfm]\n",
    "        columns = [col for col in dge_X.columns if col in pgmap.index and pgmap.loc[col] and not pgmap.loc[col].isspace()]\n",
    "        dge_X = dge_X[columns]\n",
    "\n",
    "    hist, bin_edges = zip(*[np.histogram(dge_X.iloc[i]) for i in range(dge_X.shape[0])])\n",
    "    uprg = [dge_X.iloc[i, np.where(dge_X.iloc[i] >= bin_edges[i][hist_bnd[0]])[0]].sort_values(ascending=False) for i in range(dge_X.shape[0])]\n",
    "    dwrg = [dge_X.iloc[i, np.where(dge_X.iloc[i] <= bin_edges[i][hist_bnd[1]])[0]].sort_values(ascending=True) for i in range(dge_X.shape[0])]\n",
    "    upr_genes, dwr_genes = [x.index.tolist() for x in uprg], [x.index.tolist() for x in dwrg]\n",
    "    upr_dges, dwr_dges = [x.tolist() for x in uprg], [x.tolist() for x in dwrg]\n",
    "    del uprg, dwrg\n",
    "\n",
    "    # Map to Gene Symbol\n",
    "    if (has_plfm):\n",
    "        pgmap = probe_gene_map[plfm]\n",
    "        upr_genes = [[[x.strip() for x in pgmap.loc[probe].split('///')] if (probe in pgmap.index) else [probe] for probe in probes] for probes in upr_genes]\n",
    "        uprg_lens = [[len(x) for x in genes] for genes in upr_genes]\n",
    "        upr_dges = [[[dge] * length for dge, length in zip(dges, lens)] for dges, lens in zip(upr_dges, uprg_lens)]\n",
    "        upr_genes = [func.flatten_list(probes) for probes in upr_genes]\n",
    "        upr_dges = [func.flatten_list(dges) for dges in upr_dges]\n",
    "        dwr_genes = [[[x.strip() for x in pgmap.loc[probe].split('///')] if (probe in pgmap.index) else [probe] for probe in probes] for probes in dwr_genes]\n",
    "        dwrg_lens = [[len(x) for x in genes] for genes in dwr_genes]\n",
    "        dwr_dges = [[[dge] * length for dge, length in zip(dges, lens)] for dges, lens in zip(dwr_dges, dwrg_lens)]\n",
    "        dwr_genes = [func.flatten_list(probes) for probes in dwr_genes]\n",
    "        dwr_dges = [func.flatten_list(dges) for dges in dwr_dges]\n",
    "    udr_genes.append(pd.DataFrame(OrderedDict([('up_regulated_genes', ['|'.join(map(str, x[:SGN_MAX_SIZE])) for x in upr_genes]), ('down_regulated_genes', ['|'.join(map(str, x[-SGN_MAX_SIZE:])) for x in dwr_genes]), ('up_regulated_dges', ['|'.join(map(str, x[:SGN_MAX_SIZE])) for x in upr_dges]), ('down_regulated_dges', ['|'.join(map(str, x[-SGN_MAX_SIZE:])) for x in dwr_dges])]), index=dge_X.index))\n",
    "    del upr_genes, dwr_genes, upr_dges, dwr_dges\n",
    "    if (has_plfm): del uprg_lens, dwrg_lens\n",
    "new_sgn_df = pd.concat([idx_sgn_df, pd.concat(udr_genes, axis=0, join='inner')], axis=1, join_axes=[idx_sgn_df.index])\n",
    "new_sgn_fpath = os.path.join(DATA_PATH, '%s_udrg.xlsx' % _RUN_LABEL)\n",
    "io.write_df(new_sgn_df, new_sgn_fpath, with_idx=True)\n",
    "new_sgn_df.to_excel(new_sgn_fpath, encoding='utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Gene Set Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dge2udrg(sgn_dge_fpaths, sgn_df, probe_gene_map, keep_unkown_probe=False, hist_bnd=(-2, 1)):\n",
    "    udr_genes = []\n",
    "    for sgn_dge_fpath in sgn_dge_fpaths:\n",
    "        sgn_dge = io.read_df(sgn_dge_fpath, with_idx=True).replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "        sgn_dge = sgn_dge.loc[[x for x in sgn_dge.index if x in sgn_df.index]]\n",
    "        if (np.all(pd.isnull(sgn_dge))): continue\n",
    "        # Filter out the probes that cannot be converted to gene symbols\n",
    "        plfm = sgn_df['platform'].loc[sgn_dge.index[0]]\n",
    "        has_plfm = probe_gene_map and probe_gene_map.has_key(plfm) and not probe_gene_map[plfm].empty\n",
    "        if (has_plfm and not keep_unkown_probe):\n",
    "            pgmap = probe_gene_map[plfm]\n",
    "            columns = [col for col in sgn_dge.columns if col in pgmap.index and pgmap.loc[col] and not pgmap.loc[col].isspace()]\n",
    "            sgn_dge = sgn_dge[columns]\n",
    "        \n",
    "        hist, bin_edges = zip(*[np.histogram(sgn_dge.iloc[i]) for i in range(sgn_dge.shape[0])])\n",
    "        uprg = [sgn_dge.iloc[i, np.where(sgn_dge.iloc[i] >= bin_edges[i][hist_bnd[0]])[0]].sort_values(ascending=False) for i in range(sgn_dge.shape[0])]\n",
    "        dwrg = [sgn_dge.iloc[i, np.where(sgn_dge.iloc[i] <= bin_edges[i][hist_bnd[1]])[0]].sort_values(ascending=True) for i in range(sgn_dge.shape[0])]\n",
    "        upr_genes, dwr_genes = [x.index.tolist() for x in uprg], [x.index.tolist() for x in dwrg]\n",
    "        upr_dges, dwr_dges = [x.tolist() for x in uprg], [x.tolist() for x in dwrg]\n",
    "        del uprg, dwrg\n",
    "\n",
    "        # Map to Gene Symbol\n",
    "        if (has_plfm):\n",
    "            pgmap = probe_gene_map[plfm]\n",
    "            upr_genes = [[[x.strip() for x in pgmap.loc[probe].split('///')] if (probe in pgmap.index) else [probe] for probe in probes] for probes in upr_genes]\n",
    "            uprg_lens = [[len(x) for x in genes] for genes in upr_genes]\n",
    "            upr_dges = [[[dge] * length for dge, length in zip(dges, lens)] for dges, lens in zip(upr_dges, uprg_lens)]\n",
    "            upr_genes = [func.flatten_list(probes) for probes in upr_genes]\n",
    "            upr_dges = [func.flatten_list(dges) for dges in upr_dges]\n",
    "            dwr_genes = [[[x.strip() for x in pgmap.loc[probe].split('///')] if (probe in pgmap.index) else [probe] for probe in probes] for probes in dwr_genes]\n",
    "            dwrg_lens = [[len(x) for x in genes] for genes in dwr_genes]\n",
    "            dwr_dges = [[[dge] * length for dge, length in zip(dges, lens)] for dges, lens in zip(dwr_dges, dwrg_lens)]\n",
    "            dwr_genes = [func.flatten_list(probes) for probes in dwr_genes]\n",
    "            dwr_dges = [func.flatten_list(dges) for dges in dwr_dges]\n",
    "        filtered_ids = []\n",
    "        for sid, uprg, dwrg in zip(sgn_dge.index, upr_genes, dwr_genes):\n",
    "            if (len(uprg) < SGN_MIN_SIZE and len(dwrg) < SGN_MIN_SIZE):\n",
    "                filtered_ids.append(sid)\n",
    "        udr_genes.append(pd.DataFrame(OrderedDict([('up_regulated_genes', ['|'.join(map(str, x[:SGN_MAX_SIZE])) for x in upr_genes]), ('down_regulated_genes', ['|'.join(map(str, x[-SGN_MAX_SIZE:])) for x in dwr_genes]), ('up_regulated_dges', ['|'.join(map(str, x[:SGN_MAX_SIZE])) for x in upr_dges]), ('down_regulated_dges', ['|'.join(map(str, x[-SGN_MAX_SIZE:])) for x in dwr_dges])]), index=sgn_dge.index).loc[[sid for sid in sgn_dge.index if sid not in filtered_ids]])\n",
    "        del upr_genes, dwr_genes, upr_dges, dwr_dges\n",
    "        if (has_plfm): del uprg_lens, dwrg_lens\n",
    "    return pd.concat(udr_genes, axis=0, join='inner')\n",
    "\n",
    "sgn_df = pd.read_excel(os.path.join(DATA_PATH, '%s.xlsx' % _RUN_LABEL))\n",
    "idx_sgn_df = sgn_df.set_index('id')\n",
    "keep_unkown_probe, hist_bnd = False, (-4, 3)\n",
    "\n",
    "numprocs = psutil.cpu_count()\n",
    "sgn_dge_fpaths = fs.listf(DGE_DATA_PATH, pattern='dge_X_.*\\.npz', full_path=True)\n",
    "task_bnd = njobs.split_1d(len(sgn_dge_fpaths), split_num=numprocs, ret_idx=True)\n",
    "# udr_genes = dge2udrg(sgn_dge_fpaths=sgn_dge_fpaths, sgn_df=idx_sgn_df, probe_gene_map=probe_gene_map, keep_unkown_probe=keep_unkown_probe, hist_bnd=hist_bnd)\n",
    "udr_genes = njobs.run_pool(dge2udrg, n_jobs=numprocs, dist_param=['sgn_dge_fpaths'], sgn_dge_fpaths=[sgn_dge_fpaths[task_bnd[i]:task_bnd[i+1]] for i in range(numprocs)], sgn_df=idx_sgn_df, probe_gene_map=probe_gene_map, keep_unkown_probe=keep_unkown_probe, hist_bnd=hist_bnd)\n",
    "new_sgn_df = pd.concat([idx_sgn_df, pd.concat(udr_genes, axis=0, join='inner')], axis=1, join_axes=[idx_sgn_df.index])\n",
    "new_sgn_fpath = os.path.join(DATA_PATH, '%s_udrg.xlsx' % _RUN_LABEL)\n",
    "io.write_df(new_sgn_df, new_sgn_fpath, with_idx=True)\n",
    "new_sgn_df.to_excel(new_sgn_fpath, encoding='utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Signature Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sgndb(groups, udrg_sgn_df, sgndb_path):\n",
    "    for geo_id, sgn_ids in groups:\n",
    "        sub_sgn_df = udrg_sgn_df.loc[sgn_ids]\n",
    "        # Combined signature database\n",
    "#         db_content = '\\n'.join(['%s\\t%s\\t%s' % (idx, ('%s:%s:%s'%(row['organism'], row['cell_type'], row[LABEL2OBJ[RUN_LABEL]])).replace(' ', '_'), '\\t'.join(row['up_regulated_genes'].split('|')+row['down_regulated_genes'].split('|'))) for idx, row in sub_sgn_df.iterrows()])\n",
    "#         fs.write_file(db_content, os.path.join(sgndb_path, '%s.gmt'%geo_id), code='utf-8')\n",
    "#         del db_content\n",
    "        \n",
    "        # Up-regulated signature database\n",
    "        up_db_content = '\\n'.join(['%s\\t%s\\t%s' % (idx, ('%s:%s:%s'%(row['organism'], row['cell_type'], row[LABEL2OBJ[RUN_LABEL]])).replace(' ', '_'), '\\t'.join(row['up_regulated_genes'].split('|'))) for idx, row in sub_sgn_df.iterrows()])\n",
    "        fs.write_file(up_db_content, os.path.join(sgndb_path, '%s_up.gmt'%geo_id), code='utf-8')\n",
    "        del up_db_content\n",
    "        # Down-regulated signature database\n",
    "        down_db_content = '\\n'.join(['%s\\t%s\\t%s' % (idx, ('%s:%s:%s'%(row['organism'], row['cell_type'], row[LABEL2OBJ[RUN_LABEL]])).replace(' ', '_'), '\\t'.join(row['down_regulated_genes'].split('|'))) for idx, row in sub_sgn_df.iterrows()])\n",
    "        fs.write_file(down_db_content, os.path.join(sgndb_path, '%s_down.gmt'%geo_id), code='utf-8')\n",
    "        del down_db_content\n",
    "#         print [len(row['up_regulated_genes'].split('|')) for idx, row in sub_sgn_df.iterrows()]\n",
    "#         print [len(row['down_regulated_genes'].split('|')) for idx, row in sub_sgn_df.iterrows()]\n",
    "\n",
    "\n",
    "fs.mkdir(SGNDB_PATH)\n",
    "udrg_sgn_df = io.read_df(os.path.join(DATA_PATH, '%s_udrg.xlsx'%RUN_LABEL.replace(' ', '_')), with_idx=True).dropna()\n",
    "groups = udrg_sgn_df.groupby('geo_id').groups.items()\n",
    "numprocs = psutil.cpu_count()\n",
    "task_bnd = njobs.split_1d(len(groups), split_num=numprocs, ret_idx=True)\n",
    "_ = njobs.run_pool(gen_sgndb, n_jobs=numprocs, dist_param=['groups'], groups=[groups[task_bnd[i]:task_bnd[i+1]] for i in range(numprocs)], udrg_sgn_df=udrg_sgn_df, sgndb_path=SGNDB_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gene Set Enrichment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def gsea(groups, udrg_sgn_df, probe_gene_map, sgndb_path, sample_path, method='signal_to_noise', permt_type='phenotype', permt_num=100, min_size=15, max_size=500, out_dir='gsea_output', keep_unkown_probe=False, fmt='xml', numprocs=1):\n",
    "    if (fmt == 'soft'):\n",
    "        from bionlp.spider import geo\n",
    "    else:\n",
    "        from bionlp.spider import geoxml as geo\n",
    "    for geo_id, sgn_ids in groups:\n",
    "        # Select the sub signature table\n",
    "        sub_sgn_df = udrg_sgn_df.loc[sgn_ids]\n",
    "        ids = sub_sgn_df['id'] if hasattr(sub_sgn_df, 'id') else sub_sgn_df.index\n",
    "        # Prepair the gene expression profile and the perturbation labels\n",
    "        pert_ids, ctrl_ids = list(set('|'.join(sub_sgn_df['pert_ids']).split('|'))), list(set('|'.join(sub_sgn_df['ctrl_ids']).split('|')))\n",
    "        pert_geo_docs, ctrl_geo_docs = geo.parse_geos([os.path.join(sample_path, '.'.join([pert_id, fmt])) for pert_id in pert_ids], view='full', type='gsm', fmt=fmt), geo.parse_geos([os.path.join(sample_path, '.'.join([ctrl_id, fmt])) for ctrl_id in ctrl_ids], view='full', type='gsm', fmt=fmt)\n",
    "        pert_ge_dfs, ctrl_ge_dfs = [geo_doc['data']['VALUE'] for geo_doc in pert_geo_docs], [geo_doc['data']['VALUE'] for geo_doc in ctrl_geo_docs]\n",
    "        pert_df, ctrl_df = pd.concat(pert_ge_dfs, axis=1, join='inner').astype('float32'), pd.concat(ctrl_ge_dfs, axis=1, join='inner').astype('float32')\n",
    "        pert_lb, ctrl_lb, class_vec = 'pert', 'ctrl', ['pert'] * pert_df.shape[1] + ['ctrl'] * ctrl_df.shape[1]\n",
    "        join_df = pd.concat([pert_df, ctrl_df], axis=1, join='inner')\n",
    "        join_df.columns = pert_ids + ctrl_ids\n",
    "        del pert_geo_docs, ctrl_geo_docs, pert_ge_dfs[:], ctrl_ge_dfs[:], pert_df, ctrl_df\n",
    "        # Map the probes to gene symbols\n",
    "        plfm = sub_sgn_df['platform'].iloc[0]\n",
    "        if (probe_gene_map and probe_gene_map.has_key(plfm) and not probe_gene_map[plfm].empty):\n",
    "            pgmap = probe_gene_map[plfm]\n",
    "            if (not keep_unkown_probe):\n",
    "                probes = [idx for idx in join_df.index if idx in pgmap.index and pgmap.loc[idx] and not pgmap.loc[idx].isspace()]\n",
    "                join_df = join_df.loc[probes]\n",
    "            join_df.index = [[x.strip() for x in pgmap.loc[probe].split('///')][0] if (probe in pgmap.index) else [probe] for probe in join_df.index]\n",
    "\n",
    "        join_df.reset_index(inplace=True)\n",
    "        join_df.rename(columns={'ID_REF': 'NAME'}, inplace=True)\n",
    "        join_df['NAME'] = join_df['NAME'].apply(str)\n",
    "        # Call the GSEA API\n",
    "#         try:\n",
    "#             if (not os.path.exists(os.path.join(out_dir,geo_id)) or (os.path.exists(os.path.join(out_dir,geo_id)) and len(fs.read_file(os.path.join(sgndb_path, '%s.gmt'%geo_id))) > len(fs.listf(os.path.join(out_dir,geo_id), pattern='.*\\.gsea\\.pdf')))):\n",
    "#                 print 'doing '+geo_id\n",
    "#                 gs_res = gp.gsea(data=join_df, gene_sets=os.path.join(sgndb_path, '%s.gmt'%geo_id), cls=class_vec, permutation_type=permt_type, permutation_num=permt_num, min_size=min_size, max_size=max_size, outdir=os.path.join(out_dir,geo_id), method=method, processes=numprocs, format='pdf')\n",
    "#         except Exception as e:\n",
    "#             print 'Error occured when conducting GSEA for up-regulated genes in %s!' % geo_id\n",
    "#             print e\n",
    "        try:\n",
    "            if (not os.path.exists(os.path.join(out_dir,geo_id+'up')) or (os.path.exists(os.path.join(out_dir,geo_id+'up')) and len(fs.read_file(os.path.join(sgndb_path, '%s_up.gmt'%geo_id))) > len(fs.listf(os.path.join(out_dir,geo_id+'up'), pattern='.*\\.gsea\\.pdf')))):\n",
    "                print 'doing '+geo_id+'_up'\n",
    "                gs_res = gp.gsea(data=join_df, gene_sets=os.path.join(sgndb_path, '%s_up.gmt'%geo_id), cls=class_vec, permutation_type=permt_type, permutation_num=permt_num, min_size=min_size, max_size=max_size, outdir=os.path.join(out_dir,geo_id+'up'), method=method, processes=numprocs, format='pdf')\n",
    "        except Exception as e:\n",
    "            print 'Error occured when conducting GSEA for up-regulated genes in %s!' % geo_id\n",
    "            print e\n",
    "        try:\n",
    "            if (not os.path.exists(os.path.join(out_dir,geo_id+'down')) or (os.path.exists(os.path.join(out_dir,geo_id+'down')) and len(fs.read_file(os.path.join(sgndb_path, '%s_down.gmt'%geo_id))) > len(fs.listf(os.path.join(out_dir,geo_id+'down'), pattern='.*\\.gsea\\.pdf')))):\n",
    "                print 'doing '+geo_id+'_down'\n",
    "                gs_res = gp.gsea(data=join_df, gene_sets=os.path.join(sgndb_path, '%s_down.gmt'%geo_id), cls=class_vec, permutation_type=permt_type, permutation_num=permt_num, min_size=min_size, max_size=max_size, outdir=os.path.join(out_dir,geo_id+'down'), method=method, processes=numprocs, format='pdf')\n",
    "        except Exception as e:\n",
    "            print 'Error occured when conducting GSEA for down-regulated genes in %s!' % geo_id\n",
    "            print e\n",
    "        del join_df\n",
    "    \n",
    "\n",
    "udrg_sgn_df = pd.read_excel(os.path.join(DATA_PATH, '%s_udrg.xlsx'%RUN_LABEL.replace(' ', '_')), index_col='id').dropna()\n",
    "# udrg_sgn_df = udrg_sgn_df[udrg_sgn_df['geo_id'] == 'GSE10809']\n",
    "method, permt_type, permt_num, keep_unkown_probe = 'signal_to_noise', 'phenotype', 100, False\n",
    "out_dir = os.path.join('gsea', method, _RUN_LABEL)\n",
    "# probe_gene_map = io.read_obj('probe_gene_map.pkl')\n",
    "\n",
    "numprocs = psutil.cpu_count()\n",
    "groups = udrg_sgn_df.groupby('geo_id').groups.items()\n",
    "task_bnd = njobs.split_1d(len(groups), split_num=numprocs, ret_idx=True)\n",
    "gsea(groups, udrg_sgn_df=udrg_sgn_df, probe_gene_map=probe_gene_map, sgndb_path=SGNDB_PATH, sample_path=SAMP_DIR, method=method, permt_type=permt_type, permt_num=permt_num, min_size=SGN_MIN_SIZE, max_size=SGN_MAX_SIZE, out_dir=out_dir, keep_unkown_probe=keep_unkown_probe, numprocs=numprocs)\n",
    "# _ = njobs.run_pool(gsea, n_jobs=numprocs, dist_param=['groups'], groups=[groups[task_bnd[i]:task_bnd[i+1]] for i in range(numprocs)], udrg_sgn_df=udrg_sgn_df, probe_gene_map=probe_gene_map, sgndb_path=SGNDB_PATH, sample_path=SAMP_DIR, method=method, permt_type=permt_type, permt_num=permt_num, min_size=SGN_MIN_SIZE, max_size=SGN_MAX_SIZE, out_dir=out_dir, keep_unkown_probe=keep_unkown_probe, numprocs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udrg_sgn_df = pd.read_excel(os.path.join(DATA_PATH, '%s_udrg.xlsx'%RUN_LABEL.replace(' ', '_')), index_col='id')\n",
    "method = 'signal_to_noise'\n",
    "out_dir = os.path.join('gsea', method, _RUN_LABEL)\n",
    "up_reports, down_reports = [], []\n",
    "\n",
    "for geo_id, sgn_ids in udrg_sgn_df.groupby('geo_id').groups.items():\n",
    "    uprep_fpath, downrep_fpath = os.path.join(out_dir, geo_id+'up', 'gseapy.gsea.phenotype.report.csv'), os.path.join(out_dir, geo_id+'down', 'gseapy.gsea.phenotype.report.csv')\n",
    "    if (os.path.exists(uprep_fpath)):\n",
    "        up_reports.append(pd.read_csv(uprep_fpath).set_index('Term')[['es','pval','fdr']].rename(columns={'es':'up_es','pval':'up_pval','fdr':'up_fdr'}))\n",
    "    if (os.path.exists(downrep_fpath)):\n",
    "        down_reports.append(pd.read_csv(downrep_fpath).set_index('Term')[['es','pval','fdr']].rename(columns={'es':'down_es','pval':'down_pval','fdr':'down_fdr'}))\n",
    "\n",
    "up_gsea_report = pd.concat(up_reports, axis=0)\n",
    "down_gsea_report = pd.concat(down_reports, axis=0)\n",
    "gsea_sgn_df = pd.concat([udrg_sgn_df, up_gsea_report, down_gsea_report], axis=1, join_axes=[udrg_sgn_df.index])\n",
    "io.write_df(gsea_sgn_df, '%s_udrg_gsea'%RUN_LABEL.replace(' ', '_'), with_idx=True)\n",
    "gsea_sgn_df.to_excel('%s_udrg_gsea.xlsx'%RUN_LABEL.replace(' ', '_'), encoding='utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the Gene Sets from WikiPathways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wkpw_species, wkpw_annots, wkpw_genes = [[] for x in range(3)]\n",
    "for fpath in fs.listf(WIKIPATHWAYS_PATH, pattern='.*\\.gmt', full_path=True):\n",
    "    lines = [l.strip('\\n').split('\\t') for l in fs.read_file(fpath)]\n",
    "    annots, genes = zip(*[(l[:2], l[2:]) for l in lines])\n",
    "    annots, genes = list(annots), list(genes)\n",
    "    wkpw_species.append(os.path.splitext(os.path.basename(fpath))[0].lower().replace(' ', '_')), wkpw_annots.append(list(annots)), wkpw_genes.append(list(genes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bionlp.spider import geoxml as geo\n",
    "from cStringIO import StringIO\n",
    "\n",
    "pred_species, pred_gses, pred_gpls, pred_annots, pred_genes = [[] for x in range(5)]\n",
    "for fpath in fs.listf(SGNDB_PATH, pattern='.*\\.gmt', full_path=True):\n",
    "    lines = [l.strip('\\n').split('\\t') for l in fs.read_file(fpath)]\n",
    "    annots, genes = zip(*[(l[:2], l[2:]) for l in lines])\n",
    "    annots, genes = list(annots), list(genes)\n",
    "    species = annots[0][1].split(':')[0].lower().replace(' ', '_')\n",
    "    gse_id = os.path.splitext(os.path.basename(fpath))[0].split('_')[0].lower().replace(' ', '_')\n",
    "    gse_doc = geo.parse_geo(os.path.join(GSE_DIR, '%s.xml'%gse_id.upper()), type='gse')\n",
    "    pred_species.append(species), pred_gses.append(gse_id), pred_gpls.append(geo.parse_geo(os.path.join(SAMP_DIR, '%s.xml'%gse_doc['samples'][0]), type='gsm')['platform']), pred_annots.append(list(annots)), pred_genes.append(list(genes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def gs_ol(species, gses, gpls, genes, ref_species, ref_genes):\n",
    "    try:\n",
    "        pgmap = io.read_obj(os.path.join(PLATFORM_PATH, 'probe_gene_map.pkl'))\n",
    "    except Exception as e:\n",
    "        pgmap = None\n",
    "    for species, gse, gpl, gss in zip(species, gses, gpls, genes):\n",
    "        has_pgmap = pgmap is not None and pgmap.has_key(gpl)\n",
    "        try:\n",
    "            spcs_idx = ref_species.index(species)\n",
    "        except ValueError as e:\n",
    "            print e\n",
    "            continue\n",
    "        for ref_gs in ref_genes[spcs_idx]:\n",
    "            for gs in gss:\n",
    "                if (len(gs) == 0 or not gs[0]): continue\n",
    "                if (has_pgmap):\n",
    "                    gs = func.flatten_list(map(lambda x: pgmap[gpl].loc[x].split(SC) if x and x in pgmap[gpl].index else x, gs))\n",
    "                    gs = [x if x.strip() != '///' else 0 for x in gs]\n",
    "                    gs = [x for x in gs if float(x) != 0]\n",
    "                gs_sim = difflib.SequenceMatcher(None, gs, ref_gs).ratio()\n",
    "                if (gs_sim > 0.2): print 'Found %f%% similar gene set with size %i in series %s' % (gs_sim, len(gs), gse)\n",
    "                    \n",
    "numprocs = psutil.cpu_count()\n",
    "task_bnd = njobs.split_1d(len(pred_gses), split_num=numprocs, ret_idx=True)\n",
    "# gs_ol(pred_species, pred_gses, pred_gpls, pred_genes, ref_species=wkpw_species, ref_genes=wkpw_genes)\n",
    "_ = njobs.run_pool(gs_ol, n_jobs=numprocs, dist_param=['species', 'gses', 'gpls', 'genes'], species=[pred_species[task_bnd[i]:task_bnd[i+1]] for i in range(numprocs)], gses=[pred_gses[task_bnd[i]:task_bnd[i+1]] for i in range(numprocs)], gpls=[pred_gpls[task_bnd[i]:task_bnd[i+1]] for i in range(numprocs)], genes=[pred_genes[task_bnd[i]:task_bnd[i+1]] for i in range(numprocs)], ref_species=wkpw_species, ref_genes=wkpw_genes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Constructed Data (DEPRECATED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GSE_ID = 'GSE48301'\n",
    "dge_X = io.read_df(os.path.join(DGE_DATA_PATH, 'dge_X_%s.npz'%GSE_ID), with_idx=True)\n",
    "dge_Y = io.read_df(os.path.join(DGE_DATA_PATH, 'dge_Y_%s.npz'%GSE_ID), with_idx=True, sparse_fmt='csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_dge_dfs=[io.read_df(os.path.join(DGE_PATH, fname), with_idx=True) for fname in ['dge_%s.npz'%x for x in range(71, 107)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(sub_dge_dfs[0].index) & set(sub_dge_dfs[5].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_mdls(tuned=False, glb_clfnames=[], **kwargs):\n",
    "    clf_names = []\n",
    "    for clf_name, clf in [\n",
    "        ('RandomForest', Pipeline([('clf', func.build_model(RandomForestClassifier, 'Classifier', 'Random Forest', mltl=True, mltp=True, n_jobs=1, random_state=0))]))\n",
    "    ]:\n",
    "        yield clf_name, clf\n",
    "        clf_names.append(clf_name)\n",
    "    if (len(glb_clfnames) < len(clf_names)):\n",
    "        del glb_clfnames[:]\n",
    "        glb_clfnames.extend(clf_names)\n",
    "\n",
    "txtclf.cross_validate(dge_X, dge_Y, gen_mdls, model_param=dict(tuned=False, glb_filtnames=[], glb_clfnames=[]), avg='micro', kfold=3, global_param=dict(comb=True, pl_names=[], pl_set=set([])), lbid=-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
